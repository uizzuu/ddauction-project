import os
from dotenv import load_dotenv
from typing import TypedDict, List, Union
from pinecone import Pinecone
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import Pinecone as LangchainPinecone
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage
from pydantic import BaseModel, Field


# --- í™˜ê²½ ì„¤ì • ë¡œë“œ ---
# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT")
INDEX_NAME = os.getenv("INDEX_NAME", "ddauction-db")


# =========================
# 1. RAG ìƒíƒœ ì •ì˜ ë° ë°ì´í„° ëª¨ë¸
# =========================

class Document(BaseModel):
    """ê²€ìƒ‰ ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í‘œí˜„í•˜ëŠ” ëª¨ë¸ (FastAPIì™€ ê³µìœ )í•¨"""
    source: str = Field(description="ë¬¸ì„œì˜ ì¶œì²˜ íŒŒì¼ ì´ë¦„ (ì˜ˆ: policy.md)")
    content: str = Field(description="ë¬¸ì„œ ì²­í¬ ë‚´ìš©")


class GraphState(TypedDict):
    """LangGraphì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬"""
    query: str  # ì‚¬ìš©ì ì§ˆë¬¸
    documents: List[Document]  # ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡
    generation: str  # LLMì˜ ìµœì¢… ìƒì„± ë‹µë³€
    error: Union[str, None]  # ì˜¤ë¥˜ ë©”ì‹œì§€


# =========================
# 2. LLM ë° Pinecone ì´ˆê¸°í™”
# =========================

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.3
)

# Pinecone í´ë¼ì´ì–¸íŠ¸ ë° ì„ë² ë”© ì´ˆê¸°í™”
retriever = None  # ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
try:
    if not all([OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENVIRONMENT]):
        raise ValueError("í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)

    # LangChain Pinecone RAG íˆ´ ì´ˆê¸°í™”
    # ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë¯€ë¡œ, 'process_embeddings.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
    vectorstore = LangchainPinecone.from_existing_index(
        index_name=INDEX_NAME,
        embedding=embeddings
    )
    # ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜ë¥¼ 3ê°œë¡œ ì„¤ì •
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    print(f"âœ… LangGraphìš© RAG Retriever ì´ˆê¸°í™” ì™„ë£Œ: {INDEX_NAME}")

except Exception as e:
    # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ retrieverëŠ” Noneìœ¼ë¡œ ìœ ì§€ë˜ì–´ retrieve_nodeì—ì„œ ì˜¤ë¥˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    print(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}. 'process_embeddings.py'ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.")


# =========================
# 3. Graph ë…¸ë“œ ì •ì˜
# =========================

def retrieve_node(state: GraphState):
    """Pineconeì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # retrieverê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì˜¤ë¥˜ ë°˜í™˜
    if retriever is None:
        return {"error": "RAG Retrieverê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. DB ì—°ê²° ë° ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”."}

    print(f"--- ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘ (Query: {state['query'][:30]}...) ---")

    try:
        # LangChain Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = retriever.invoke(state["query"])

        documents = []
        for doc in retrieved_docs:
            # LangGraph ìƒíƒœì— ë§ê²Œ Document ëª¨ë¸ë¡œ ë³€í™˜
            documents.append(Document(
                source=doc.metadata.get('source', 'N/A'),
                content=doc.page_content
            ))

        print(f"--- âœ… {len(documents)}ê°œì˜ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ ---")
        return {"documents": documents, "generation": ""}

    except Exception as e:
        print(f"--- âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} ---")
        return {"documents": [], "error": str(e)}


def generate_node(state: GraphState):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""

    documents = state["documents"]
    query = state["query"]

    if not documents:
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ë‹µë³€ì„ ë°˜í™˜
        return {"generation": "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë•…ë•…ì˜¥ì…˜ ê·œì • ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    print("--- ğŸ§  LLM ë‹µë³€ ìƒì„± ì‹œì‘ ---")

    # RAG í”„ë¡¬í”„íŠ¸ êµ¬ì¶•
    context = "\n---\n".join([f"ì¶œì²˜: {d.source}\në‚´ìš©: {d.content}" for d in documents])

    system_prompt = (
        "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì‡¼í•‘ëª°Â·ì¤‘ê³ ê±°ë˜Â·ê²½ë§¤ í”Œë«í¼ ê·œì • ì•ˆë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.  \n"
        "ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ë‚´ìš©ì— ëŒ€í•´ **ì˜¤ì§ ì œê³µëœ RAG ë¬¸ì„œ(Context)**ë§Œ ì°¸ê³ í•˜ì—¬ ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
        "[ì§€ì¼œì•¼ í•  ê·œì¹™]\n"
        "1. ë¬¸ì„œì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ìˆì„ ë•Œë§Œ ë‹µë³€í•©ë‹ˆë‹¤.\n"
        "2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‹µí•©ë‹ˆë‹¤:  \n"
        "\"ì œê³µëœ ê·œì • ë¬¸ì„œì— í•´ë‹¹ ë‚´ìš©ì´ ëª…ì‹œë˜ì–´ ìˆì§€ ì•Šì•„ ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n"
        "3. ì¶”ì¸¡, ì¼ë°˜ ìƒì‹, ì¸í„°ë„· ì •ë³´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
        "4. ë¬¸ì„œ ì¶œì²˜ íŒŒì¼ëª…(ì˜ˆ: policy.md)ì€ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "5. ë‹µë³€ í˜•ì‹:\n"
        "   - í•µì‹¬ ìš”ì•½ 1~2ì¤„\n"
        "   - ê·œì • ìƒì„¸ ë‚´ìš© (ë¬¸ì„œ ê¸°ë°˜)\n"
        "   - í•„ìš” ì‹œ ì¶”ê°€ ì£¼ì˜ì‚¬í•­ (ë¬¸ì„œì— ìˆì„ ê²½ìš°ë§Œ)\n"
        "6. ë¬¸ì„œê°€ ì—¬ëŸ¬ ê°œì´ë©´ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì¼ê´€ì„± ìˆê²Œ ë‹µë³€í•©ë‹ˆë‹¤.\n\n"
        "í™˜ê²½:\n"
        "- ì¼ë°˜ ì‡¼í•‘ëª° ìƒí’ˆ íŒë§¤\n"
        "- ì¤‘ê³  ê°œì¸ ê°„ ê±°ë˜\n"
        "- ê²½ë§¤ ìƒí’ˆ íŒë§¤\n"
        "- ìŠ¤í† ì–´ íŒë§¤\n"
        "- ë°°ì†¡, ë°˜í’ˆ, í™˜ë¶ˆ, í˜ë„í‹° ë“± ì‡¼í•‘ëª° ê´€ë ¨ ê·œì •\n"
        "ì •í™•í•˜ê³  ì¹œì ˆí•˜ë©° ê·œì • ì¤‘ì‹¬ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."
    )

    prompt_template = f"{system_prompt}\n\n[ìë£Œ]\n{context}\n\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{query}"

    try:
        response = llm.invoke([HumanMessage(content=prompt_template)])
        print("--- âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ ---")
        return {"generation": response.content}

    except Exception as e:
        print(f"--- âŒ LLM ìƒì„± ì˜¤ë¥˜: {e} ---")
        return {"generation": "ì£„ì†¡í•©ë‹ˆë‹¤. LLM ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}


# =========================
# 4. Graph ì •ì˜ ë° ì»´íŒŒì¼
# =========================
#
# GraphStateë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ Graph ìƒì„±
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("generate", generate_node)

# ê·¸ë˜í”„ ì‹œì‘ì  ì„¤ì •
workflow.set_entry_point("retrieve")

# íë¦„ ì •ì˜: ê²€ìƒ‰ ë…¸ë“œ -> ìƒì„± ë…¸ë“œ -> ì¢…ë£Œ
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

# Graph ì»´íŒŒì¼ (ì‹¤ì œë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°ì²´)
rag_app = workflow.compile()


# =========================
# 5. LangGraph API í˜¸ì¶œ í•¨ìˆ˜
# =========================
def run_langgraph_rag(query: str):
    """ì»´íŒŒì¼ëœ LangGraph RAG ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    initial_state = {"query": query, "documents": [], "generation": "", "error": None}

    # LangGraph ì‹¤í–‰
    final_state = rag_app.invoke(initial_state)

    # ê²°ê³¼ ë°˜í™˜
    if final_state.get("error"):
        # ì˜¤ë¥˜ê°€ ìˆì„ ê²½ìš° ì‘ë‹µì€ ì˜¤ë¥˜ ë©”ì‹œì§€, ë¬¸ì„œëŠ” ë¹ˆ ëª©ë¡
        return {"response": final_state["error"], "documents": []}

    return {
        "response": final_state["generation"],
        "documents": final_state["documents"]
    }

# ì´ íŒŒì¼ì˜ 'rag_app' ì¸ìŠ¤í„´ìŠ¤ëŠ” FastAPI íŒŒì¼ì—ì„œ ì„í¬íŠ¸í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.